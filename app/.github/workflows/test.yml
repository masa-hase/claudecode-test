name: Test Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: '20'

jobs:
  # ä¸¦åˆ—å®Ÿè¡Œ1: ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1

      - name: Install dependencies
        run: bun install
        working-directory: ./app

      - name: Run unit tests
        run: npm test -- --coverage --outputFile=unit-test-results.json --json
        working-directory: ./app

      - name: Upload unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: |
            app/unit-test-results.json
            app/coverage/

  # ä¸¦åˆ—å®Ÿè¡Œ2: E2Eãƒ†ã‚¹ãƒˆ
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1

      - name: Install dependencies
        run: bun install
        working-directory: ./app

      - name: Install Playwright browsers
        run: npx playwright install --with-deps
        working-directory: ./app

      - name: Build application
        run: npm run build
        working-directory: ./app

      - name: Run E2E tests
        run: npm run test:e2e -- --reporter=json --output-file=e2e-test-results.json
        working-directory: ./app

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            app/e2e-test-results.json
            app/test-results/
            app/playwright-report/

  # ä¸¦åˆ—å®Ÿè¡Œ3: ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1

      - name: Install dependencies
        run: bun install
        working-directory: ./app

      - name: Run Biome checks
        run: bun run check --reporter=json --output-file=biome-results.json
        working-directory: ./app
        continue-on-error: true

      - name: Upload code quality results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: code-quality-results
          path: app/biome-results.json

  # ãƒ†ã‚¹ãƒˆçµæœã®çµ±åˆã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
  generate-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, e2e-tests, code-quality]
    if: always()

    outputs:
      test-status: ${{ steps.evaluate.outputs.test-status }}
      report-summary: ${{ steps.generate-report.outputs.summary }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Generate comprehensive test report
        id: generate-report
        run: |
          # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
          cat > generate_report.js << 'EOF'
          const fs = require('fs');
          const path = require('path');

          function generateReport() {
            const report = {
              timestamp: new Date().toISOString(),
              commit: process.env.GITHUB_SHA,
              branch: process.env.GITHUB_REF_NAME,
              workflow: process.env.GITHUB_WORKFLOW,
              runId: process.env.GITHUB_RUN_ID,
              results: {}
            };

            let allTestsPassed = true;
            let summary = [];

            // ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆçµæœ
            try {
              if (fs.existsSync('./unit-test-results/unit-test-results.json')) {
                const unitResults = JSON.parse(fs.readFileSync('./unit-test-results/unit-test-results.json', 'utf8'));
                report.results.unit = {
                  success: unitResults.success,
                  numTotalTests: unitResults.numTotalTests,
                  numPassedTests: unitResults.numPassedTests,
                  numFailedTests: unitResults.numFailedTests,
                  testResults: unitResults.testResults
                };
                summary.push(`âœ… Unit Tests: ${unitResults.numPassedTests}/${unitResults.numTotalTests} passed`);
                if (!unitResults.success) allTestsPassed = false;
              } else {
                report.results.unit = { error: 'Results not found' };
                summary.push(`âŒ Unit Tests: Results not found`);
                allTestsPassed = false;
              }
            } catch (error) {
              report.results.unit = { error: error.message };
              summary.push(`âŒ Unit Tests: Error reading results`);
              allTestsPassed = false;
            }

            // E2Eãƒ†ã‚¹ãƒˆçµæœ
            try {
              if (fs.existsSync('./e2e-test-results/e2e-test-results.json')) {
                const e2eResults = JSON.parse(fs.readFileSync('./e2e-test-results/e2e-test-results.json', 'utf8'));
                const totalTests = e2eResults.stats?.expected || 0;
                const passedTests = e2eResults.stats?.passed || 0;
                const failedTests = e2eResults.stats?.failed || 0;
                
                report.results.e2e = {
                  success: failedTests === 0 && totalTests > 0,
                  totalTests,
                  passedTests,
                  failedTests,
                  suites: e2eResults.suites
                };
                summary.push(`âœ… E2E Tests: ${passedTests}/${totalTests} passed`);
                if (failedTests > 0) allTestsPassed = false;
              } else {
                report.results.e2e = { error: 'Results not found' };
                summary.push(`âŒ E2E Tests: Results not found`);
                allTestsPassed = false;
              }
            } catch (error) {
              report.results.e2e = { error: error.message };
              summary.push(`âŒ E2E Tests: Error reading results`);
              allTestsPassed = false;
            }

            // ã‚³ãƒ¼ãƒ‰å“è³ªçµæœ
            try {
              if (fs.existsSync('./code-quality-results/biome-results.json')) {
                const biomeResults = JSON.parse(fs.readFileSync('./code-quality-results/biome-results.json', 'utf8'));
                report.results.codeQuality = biomeResults;
                const hasErrors = biomeResults.summary?.errorCount > 0;
                summary.push(`${hasErrors ? 'âš ï¸' : 'âœ…'} Code Quality: ${biomeResults.summary?.errorCount || 0} errors`);
                if (hasErrors) allTestsPassed = false;
              } else {
                report.results.codeQuality = { error: 'Results not found' };
                summary.push(`âŒ Code Quality: Results not found`);
              }
            } catch (error) {
              report.results.codeQuality = { error: error.message };
              summary.push(`âŒ Code Quality: Error reading results`);
            }

            // ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            fs.writeFileSync('./test-report.json', JSON.stringify(report, null, 2));

            // ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            const summaryText = summary.join('\n');
            console.log('SUMMARY=' + summaryText);
            console.log('STATUS=' + (allTestsPassed ? 'success' : 'failure'));
            
            return { summary: summaryText, status: allTestsPassed ? 'success' : 'failure' };
          }

          const result = generateReport();
          EOF

          # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Ÿè¡Œ
          node generate_report.js
          
          # GitHub Actions ã®å‡ºåŠ›ã«è¨­å®š
          echo "summary<<EOF" >> $GITHUB_OUTPUT
          cat test-report.json | jq -r '.results | to_entries | map("\(.key): \(.value)") | join("\n")'
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Evaluate test status
        id: evaluate
        run: |
          # å„ã‚¸ãƒ§ãƒ–ã®çµæœã‚’ç¢ºèª
          UNIT_STATUS="${{ needs.unit-tests.result }}"
          E2E_STATUS="${{ needs.e2e-tests.result }}"
          QUALITY_STATUS="${{ needs.code-quality.result }}"
          
          echo "Unit Tests: $UNIT_STATUS"
          echo "E2E Tests: $E2E_STATUS"  
          echo "Code Quality: $QUALITY_STATUS"
          
          if [[ "$UNIT_STATUS" == "success" && "$E2E_STATUS" == "success" ]]; then
            echo "test-status=success" >> $GITHUB_OUTPUT
          else
            echo "test-status=failure" >> $GITHUB_OUTPUT
          fi

      - name: Upload test report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-report
          path: test-report.json

  # æˆåŠŸæ™‚: PRã‚’ä½œæˆ
  create-success-pr:
    name: Create Success PR
    runs-on: ubuntu-latest
    needs: [generate-report]
    if: needs.generate-report.outputs.test-status == 'success' && github.event_name == 'push' && github.ref_name != 'main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download test report
        uses: actions/download-artifact@v4
        with:
          name: test-report

      - name: Create PR for successful tests
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          title: "âœ… All Tests Passed - Ready for Review"
          body: |
            ## ğŸ‰ Test Results Summary
            
            All tests have passed successfully! This PR is automatically created to merge the changes.
            
            ### Test Results
            ```
            ${{ needs.generate-report.outputs.report-summary }}
            ```
            
            ### Details
            - **Commit**: ${{ github.sha }}
            - **Branch**: ${{ github.ref_name }}
            - **Workflow Run**: [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ### Next Steps
            - Review the changes
            - Merge if everything looks good
            
            ---
            ğŸ¤– Automatically created by GitHub Actions
          branch: auto-pr-${{ github.ref_name }}-${{ github.run_id }}
          base: main
          labels: |
            automated
            tests-passed
            ready-for-review

  # å¤±æ•—æ™‚: Issueã‚’ä½œæˆ
  create-failure-issue:
    name: Create Failure Issue
    runs-on: ubuntu-latest
    needs: [generate-report]
    if: needs.generate-report.outputs.test-status == 'failure'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test report
        uses: actions/download-artifact@v4
        with:
          name: test-report

      - name: Create issue for test failures
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
            let reportContent = 'Test report not available';
            try {
              const report = JSON.parse(fs.readFileSync('./test-report.json', 'utf8'));
              reportContent = JSON.stringify(report, null, 2);
            } catch (error) {
              reportContent = `Error reading test report: ${error.message}`;
            }

            const issueBody = `## âŒ Test Failure Report

### Summary
Tests have failed in the latest run. Please investigate and fix the issues.

### Test Results
\`\`\`
${{ needs.generate-report.outputs.report-summary }}
\`\`\`

### Failure Details
<details>
<summary>Click to view detailed test report</summary>

\`\`\`json
${reportContent}
\`\`\`
</details>

### Context
- **Commit**: ${{ github.sha }}
- **Branch**: ${{ github.ref_name }}
- **Workflow Run**: [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
- **Triggered by**: ${{ github.event_name }}

### Action Items
- [ ] Review failed tests
- [ ] Fix failing tests
- [ ] Re-run tests to verify fixes
- [ ] Close this issue when resolved

---
ğŸ¤– Automatically created by GitHub Actions`;

            // æ—¢å­˜ã®æœªè§£æ±ºissueæ¤œç´¢
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'test-failure,automated'
            });

            if (existingIssues.data.length > 0) {
              // æ—¢å­˜issueã«ã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssues.data[0].number,
                body: `## ğŸ”„ New Test Failure\n\n${issueBody}`
              });
            } else {
              // æ–°ã—ã„issueä½œæˆ
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `âŒ Test Failures - ${new Date().toISOString().split('T')[0]}`,
                body: issueBody,
                labels: ['test-failure', 'automated', 'bug', 'priority-high']
              });
            }

  # é€šçŸ¥
  notify-results:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [generate-report, create-success-pr, create-failure-issue]
    if: always()

    steps:
      - name: Report workflow status
        run: |
          echo "## Workflow Completed"
          echo "Test Status: ${{ needs.generate-report.outputs.test-status }}"
          echo "Summary: ${{ needs.generate-report.outputs.report-summary }}"
          
          if [[ "${{ needs.generate-report.outputs.test-status }}" == "success" ]]; then
            echo "âœ… All tests passed - PR created for review"
          else
            echo "âŒ Tests failed - Issue created for investigation"
          fi